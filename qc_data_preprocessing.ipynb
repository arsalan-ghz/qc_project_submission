{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba46fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 95 CSV files\n",
      "Merged CSV saved to: agnmass_dump/reverb_tables/qc_project_merged_data.csv\n",
      "Total number of rows in merged dataset: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_199151/1414516086.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(dfs, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Directory containing per-object CSVs\n",
    "DATA_DIR = Path(\"agnmass_dump/reverb_tables\")\n",
    "\n",
    "# Collect all CSV files\n",
    "csv_files = sorted(DATA_DIR.glob(\"details_varname_*_reverb_with_meta.csv\"))\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "# Read and concatenate\n",
    "dfs = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f, sep=\";\")\n",
    "    dfs.append(df)\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save merged CSV\n",
    "OUTPUT_PATH = DATA_DIR / \"qc_project_merged_data.csv\"\n",
    "merged_df.to_csv(OUTPUT_PATH, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Merged CSV saved to: {OUTPUT_PATH}\")\n",
    "print(f\"Total number of rows in merged dataset: {merged_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eab65f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_199151/991415248.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_df_clean = merged_df.replace(missing_markers, np.nan)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mbh_rm_modeling             196\n",
       "mbh_rm_ref                  196\n",
       "alternate_names             171\n",
       "τJAV Ref.                   147\n",
       "τJAV (days)                 147\n",
       "log LAGN,5100 (ergs s-1)    143\n",
       "L Ref.                      143\n",
       "τpeak (days)                 71\n",
       "σline (km s-1)               70\n",
       "FWHM (km s-1)                67\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace common missing-value markers with NaN\n",
    "missing_markers = [\n",
    "    \"...\", \"…\", \"\", \" \", \"None\", \"none\", \"NULL\", \"null\"\n",
    "]\n",
    "\n",
    "merged_df_clean = merged_df.replace(missing_markers, np.nan)\n",
    "\n",
    "# Optional: show how many NaNs per column (sanity check)\n",
    "na_summary = merged_df_clean.isna().sum().sort_values(ascending=False)\n",
    "na_summary.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907899ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose identifier (varname is safest)\n",
    "# ID_COL = \"varname\"\n",
    "\n",
    "# total_objects = merged_df_clean[ID_COL].nunique()\n",
    "# print(f\"Total unique objects: {total_objects}\\n\")\n",
    "\n",
    "# def report_missing(label_col):\n",
    "#     \"\"\"\n",
    "#     Reports how many unique objects are missing a given label.\n",
    "#     \"\"\"\n",
    "#     # For each object, check if ALL rows are NaN for that label\n",
    "#     missing_by_object = (\n",
    "#         merged_df_clean\n",
    "#         .groupby(ID_COL)[label_col]\n",
    "#         .apply(lambda s: s.isna().all())\n",
    "#     )\n",
    "\n",
    "#     num_missing = missing_by_object.sum()\n",
    "#     percent_missing = 100 * num_missing / total_objects\n",
    "\n",
    "#     print(f\"{label_col}:\")\n",
    "#     print(f\"  Objects missing value : {num_missing}\")\n",
    "#     print(f\"  Percentage missing    : {percent_missing:.2f}%\")\n",
    "#     print(f\"  Objects with value    : {total_objects - num_missing}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Run reports\n",
    "# report_missing(\"mbh_hbeta_only\")\n",
    "# report_missing(\"mbh_all_lines\")\n",
    "# report_missing(\"mbh_rm_modeling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee89c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: agnmass_dump/reverb_tables/qc_project_merged_data.csv\n",
      "Shape: (299, 25)\n",
      "Added uncertainty numeric columns for 6 measurement columns.\n",
      "Added MBH parsed columns (mantissa/errors/exponent/msun/log10) for: ['mbh_hbeta_only', 'mbh_all_lines', 'mbh_rm_modeling']\n",
      "Parsed JD range into jd_start/jd_end/jd_span/jd_mid using: JD Range (days)\n",
      "\n",
      "Done. Cleaned + feature-augmented dataframe:\n",
      "Shape: (299, 77)\n",
      "Cleaned data saved to: agnmass_dump/reverb_tables/qc_project_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Load merged CSV ----------\n",
    "path = Path(\"agnmass_dump/reverb_tables/qc_project_merged_data.csv\")\n",
    "\n",
    "# Your files are usually ';' delimited (German Excel-friendly). This tries ';' first, then ','.\n",
    "try:\n",
    "    df = pd.read_csv(path, sep=\";\", encoding=\"utf-8-sig\")\n",
    "except Exception:\n",
    "    df = pd.read_csv(path, sep=\",\", encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Loaded:\", path)\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# ---------- Helpers for robust missing detection ----------\n",
    "# Matches ellipsis forms like:\n",
    "#   \"...\" , \"…\", \". . .\", \".  .   .\", \"... (10 ^7 M_sun)\", \"… something\"\n",
    "ELLIPSIS_START_RE = r\"^\\s*(?:\\.\\s*){3,}.*$|^\\s*….*$\"\n",
    "ELLIPSIS_ONLY_RE  = r\"^\\s*(?:\\.\\s*){3,}\\s*$|^\\s*…\\s*$\"\n",
    "\n",
    "# ---------- (1) Global missing cleanup ----------\n",
    "# 1A) Whitespace-only -> NaN\n",
    "df = df.replace(to_replace=r\"^\\s*$\", value=np.nan, regex=True)\n",
    "\n",
    "# 1B) Ellipsis-only cells -> NaN\n",
    "df = df.replace(to_replace=ELLIPSIS_ONLY_RE, value=np.nan, regex=True)\n",
    "\n",
    "# 1C) (Optional global) cells starting with ellipsis -> NaN\n",
    "# This is safe for this dataset and catches embedded missing like \"... (10^7 ...)\"\n",
    "df = df.replace(to_replace=ELLIPSIS_START_RE, value=np.nan, regex=True)\n",
    "\n",
    "# 1D) Strip string cells (helps matching + cleaner viewing)\n",
    "for c in df.columns:\n",
    "    if df[c].dtype == \"object\":\n",
    "        df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# ---------- (2) Generic uncertainty parser (tau/sigma/FWHM/logL etc.) ----------\n",
    "def parse_value_and_uncertainty(x):\n",
    "    \"\"\"\n",
    "    Returns (val, plus, minus) as floats from strings like:\n",
    "      '0.74 (+0.49/-0.49)'\n",
    "      '3001 (+/- 277)'\n",
    "      '43.70 (+/- 0.06)'\n",
    "    If only a number exists: (val, nan, nan)\n",
    "    If missing/unparseable: (nan, nan, nan)\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "    s = str(x)\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # If it still begins with ellipsis for any reason, treat as missing\n",
    "    if re.match(ELLIPSIS_START_RE, s):\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "    # First number = central value\n",
    "    m0 = re.search(r\"([-+]?\\d+(?:\\.\\d+)?)\", s)\n",
    "    if not m0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    val = float(m0.group(1))\n",
    "\n",
    "    # Asymmetric: (+a/-b)\n",
    "    m = re.search(r\"\\(\\s*\\+\\s*([\\d.]+)\\s*/\\s*-\\s*([\\d.]+)\\s*\\)\", s)\n",
    "    if m:\n",
    "        return (val, float(m.group(1)), float(m.group(2)))\n",
    "\n",
    "    # Symmetric: (+/- a)\n",
    "    m = re.search(r\"\\(\\s*\\+/-\\s*([\\d.]+)\\s*\\)\", s)\n",
    "    if m:\n",
    "        a = float(m.group(1))\n",
    "        return (val, a, a)\n",
    "\n",
    "    return (val, np.nan, np.nan)\n",
    "\n",
    "def add_uncertainty_columns(df, col):\n",
    "    vals = df[col].apply(\n",
    "        lambda x: pd.Series(\n",
    "            parse_value_and_uncertainty(x),\n",
    "            index=[f\"{col}_val\", f\"{col}_plus\", f\"{col}_minus\"]\n",
    "        )\n",
    "    )\n",
    "    return pd.concat([df, vals], axis=1)\n",
    "\n",
    "# Measurement columns to parse (avoid refs)\n",
    "mbh_cols = [\"mbh_hbeta_only\", \"mbh_all_lines\", \"mbh_rm_modeling\"]\n",
    "candidate_cols = []\n",
    "\n",
    "for c in df.columns:\n",
    "    if c in mbh_cols:\n",
    "        continue\n",
    "\n",
    "    cname = str(c)\n",
    "    cname_lower = cname.lower()\n",
    "\n",
    "    # skip reference columns\n",
    "    if \"ref\" in cname_lower:\n",
    "        continue\n",
    "\n",
    "    # typical measurement columns contain these substrings\n",
    "    if any(k in cname for k in [\"τ\", \"sigma\", \"σ\", \"FWHM\", \"log L\", \"logL\", \"L_AGN\", \"LAGN\"]):\n",
    "        candidate_cols.append(c)\n",
    "\n",
    "for c in candidate_cols:\n",
    "    df = add_uncertainty_columns(df, c)\n",
    "\n",
    "print(f\"Added uncertainty numeric columns for {len(candidate_cols)} measurement columns.\")\n",
    "\n",
    "# ---------- (3) MBH parser: mantissa + uncertainty + exponent + log10 mass ----------\n",
    "def parse_mbh_string(x):\n",
    "    \"\"\"\n",
    "    Parses strings like:\n",
    "      '1.961 (^+ 0.516 / _- 0.392) (10 ^7 M _sun)'\n",
    "      '1.699 (+ 0.171/- 0.164) (10^7 M_sun)'\n",
    "\n",
    "    Returns:\n",
    "      mantissa, plus, minus, exponent\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "    s = str(x)\n",
    "    s = s.replace(\"\\n\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # CRITICAL: if MBH string starts with ellipsis, it is missing (even if it contains \"10^7\")\n",
    "    if re.match(ELLIPSIS_START_RE, s):\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "    # Central mantissa = first float\n",
    "    m0 = re.search(r\"([-+]?\\d+(?:\\.\\d+)?)\", s)\n",
    "    if not m0:\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "    mantissa = float(m0.group(1))\n",
    "\n",
    "    # Normalize sup/sub style artifacts\n",
    "    norm = s.replace(\"^+\", \"+\").replace(\"_-\", \"-\")\n",
    "    norm = norm.replace(\"(^\", \"(\").replace(\"/ _\", \"/ -\").replace(\"( ^\", \"(\")\n",
    "\n",
    "    plus = minus = np.nan\n",
    "\n",
    "    m = re.search(r\"\\(\\s*\\+\\s*([\\d.]+)\\s*/\\s*-\\s*([\\d.]+)\\s*\\)\", norm)\n",
    "    if m:\n",
    "        plus, minus = float(m.group(1)), float(m.group(2))\n",
    "    else:\n",
    "        m = re.search(r\"\\(\\s*\\+/-\\s*([\\d.]+)\\s*\\)\", norm)\n",
    "        if m:\n",
    "            plus = minus = float(m.group(1))\n",
    "\n",
    "    # Exponent from '10 ^7' or '10^7'\n",
    "    exp = np.nan\n",
    "    mexp = re.search(r\"10\\s*\\^?\\s*([+-]?\\d+)\", norm)\n",
    "    if mexp:\n",
    "        exp = float(mexp.group(1))\n",
    "\n",
    "    return (mantissa, plus, minus, exp)\n",
    "\n",
    "def add_mbh_columns(df, col):\n",
    "    # extra pre-clean: convert any ellipsis-starting strings to NaN in MBH columns\n",
    "    df[col] = df[col].replace(to_replace=ELLIPSIS_START_RE, value=np.nan, regex=True)\n",
    "\n",
    "    out = df[col].apply(\n",
    "        lambda x: pd.Series(\n",
    "            parse_mbh_string(x),\n",
    "            index=[f\"{col}_mantissa\", f\"{col}_plus_mant\", f\"{col}_minus_mant\", f\"{col}_exp\"]\n",
    "        )\n",
    "    )\n",
    "    df = pd.concat([df, out], axis=1)\n",
    "\n",
    "    mant = df[f\"{col}_mantissa\"]\n",
    "    exp = df[f\"{col}_exp\"]\n",
    "    plus = df[f\"{col}_plus_mant\"]\n",
    "    minus = df[f\"{col}_minus_mant\"]\n",
    "\n",
    "    # Only compute when exp is present\n",
    "    factor = 10 ** exp\n",
    "\n",
    "    df[f\"{col}_msun_val\"] = mant * factor\n",
    "    df[f\"{col}_msun_plus\"] = (mant + plus) * factor\n",
    "    df[f\"{col}_msun_minus\"] = (mant - minus) * factor\n",
    "\n",
    "    # log10(M/Msun)\n",
    "    df[f\"{col}_log10_val\"] = np.log10(mant) + exp\n",
    "    df[f\"{col}_log10_plus\"] = np.log10((mant + plus) * factor) - df[f\"{col}_log10_val\"]\n",
    "    df[f\"{col}_log10_minus\"] = df[f\"{col}_log10_val\"] - np.log10((mant - minus) * factor)\n",
    "\n",
    "    return df\n",
    "\n",
    "for col in mbh_cols:\n",
    "    if col in df.columns:\n",
    "        df = add_mbh_columns(df, col)\n",
    "\n",
    "print(\"Added MBH parsed columns (mantissa/errors/exponent/msun/log10) for:\", [c for c in mbh_cols if c in df.columns])\n",
    "\n",
    "# ---------- (4) Parse JD Range ----------\n",
    "jd_cols = [c for c in df.columns if \"JD Range\" in str(c)]\n",
    "if jd_cols:\n",
    "    jd_col = jd_cols[0]\n",
    "\n",
    "    def parse_jd_range(x):\n",
    "        if pd.isna(x):\n",
    "            return (np.nan, np.nan, np.nan, np.nan)\n",
    "        s = str(x).strip()\n",
    "        m = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*-\\s*(\\d+(?:\\.\\d+)?)\", s)\n",
    "        if not m:\n",
    "            return (np.nan, np.nan, np.nan, np.nan)\n",
    "        a, b = float(m.group(1)), float(m.group(2))\n",
    "        return (a, b, b - a, 0.5 * (a + b))\n",
    "\n",
    "    jd_out = df[jd_col].apply(\n",
    "        lambda x: pd.Series(parse_jd_range(x), index=[\"jd_start\", \"jd_end\", \"jd_span\", \"jd_mid\"])\n",
    "    )\n",
    "    df = pd.concat([df, jd_out], axis=1)\n",
    "    print(\"Parsed JD range into jd_start/jd_end/jd_span/jd_mid using:\", jd_col)\n",
    "\n",
    "# ---------- Final ----------\n",
    "merged_df_clean = df\n",
    "\n",
    "print(\"\\nDone. Cleaned + feature-augmented dataframe:\")\n",
    "print(\"Shape:\", merged_df_clean.shape)\n",
    "\n",
    "# ---------- Save cleaned & augmented data ----------\n",
    "OUT_PATH = Path(\"agnmass_dump/reverb_tables/qc_project_cleaned_data.csv\")\n",
    "merged_df_clean.to_csv(OUT_PATH, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Cleaned data saved to: {OUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1bece70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the cleaned data:\n",
      "Index(['Line', 'JD Range (days)', 'τJAV (days)', 'τJAV Ref.', 'τcent (days)',\n",
      "       'τpeak (days)', 'σline (km s-1)', 'FWHM (km s-1)', 'RM Ref(s)',\n",
      "       'log LAGN,5100 (ergs s-1)', 'L Ref.', 'varname', 'object_name',\n",
      "       'alternate_names', 'ra', 'dec', 'z', 'dl_mpc', 'da_mpc', 'f_used',\n",
      "       'mbh_hbeta_only', 'mbh_all_lines', 'mbh_rm_modeling', 'mbh_rm_ref',\n",
      "       'source_url', 'τJAV (days)_val', 'τJAV (days)_plus',\n",
      "       'τJAV (days)_minus', 'τcent (days)_val', 'τcent (days)_plus',\n",
      "       'τcent (days)_minus', 'τpeak (days)_val', 'τpeak (days)_plus',\n",
      "       'τpeak (days)_minus', 'σline (km s-1)_val', 'σline (km s-1)_plus',\n",
      "       'σline (km s-1)_minus', 'FWHM (km s-1)_val', 'FWHM (km s-1)_plus',\n",
      "       'FWHM (km s-1)_minus', 'log LAGN,5100 (ergs s-1)_val',\n",
      "       'log LAGN,5100 (ergs s-1)_plus', 'log LAGN,5100 (ergs s-1)_minus',\n",
      "       'mbh_hbeta_only_mantissa', 'mbh_hbeta_only_plus_mant',\n",
      "       'mbh_hbeta_only_minus_mant', 'mbh_hbeta_only_exp',\n",
      "       'mbh_hbeta_only_msun_val', 'mbh_hbeta_only_msun_plus',\n",
      "       'mbh_hbeta_only_msun_minus', 'mbh_hbeta_only_log10_val',\n",
      "       'mbh_hbeta_only_log10_plus', 'mbh_hbeta_only_log10_minus',\n",
      "       'mbh_all_lines_mantissa', 'mbh_all_lines_plus_mant',\n",
      "       'mbh_all_lines_minus_mant', 'mbh_all_lines_exp',\n",
      "       'mbh_all_lines_msun_val', 'mbh_all_lines_msun_plus',\n",
      "       'mbh_all_lines_msun_minus', 'mbh_all_lines_log10_val',\n",
      "       'mbh_all_lines_log10_plus', 'mbh_all_lines_log10_minus',\n",
      "       'mbh_rm_modeling_mantissa', 'mbh_rm_modeling_plus_mant',\n",
      "       'mbh_rm_modeling_minus_mant', 'mbh_rm_modeling_exp',\n",
      "       'mbh_rm_modeling_msun_val', 'mbh_rm_modeling_msun_plus',\n",
      "       'mbh_rm_modeling_msun_minus', 'mbh_rm_modeling_log10_val',\n",
      "       'mbh_rm_modeling_log10_plus', 'mbh_rm_modeling_log10_minus', 'jd_start',\n",
      "       'jd_end', 'jd_span', 'jd_mid'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read agnmass_dump/reverb_tables/qc_project_cleaned_data.csv and print the name of all its columns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"agnmass_dump/reverb_tables/qc_project_cleaned_data.csv\")\n",
    "df = pd.read_csv(path, sep=\";\", encoding=\"utf-8-sig\")\n",
    "print(\"Columns in the cleaned data:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408c8e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: agnmass_dump/reverb_tables/qc_project_cleaned_data.csv\n",
      "Shape: (299, 77)\n",
      "\n",
      "Total unique objects: 86\n",
      "\n",
      "mbh_hbeta_only_log10_val:\n",
      "  Objects missing value : 19\n",
      "  Percentage missing    : 22.09%\n",
      "  Objects with value    : 67\n",
      "------------------------------------------------------------\n",
      "mbh_all_lines_log10_val:\n",
      "  Objects missing value : 16\n",
      "  Percentage missing    : 18.60%\n",
      "  Objects with value    : 70\n",
      "------------------------------------------------------------\n",
      "mbh_rm_modeling_log10_val:\n",
      "  Objects missing value : 71\n",
      "  Percentage missing    : 82.56%\n",
      "  Objects with value    : 15\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the cleaned & augmented dataset\n",
    "CLEAN_PATH = Path(\"agnmass_dump/reverb_tables/qc_project_cleaned_data.csv\")\n",
    "\n",
    "# Your cleaned CSV was saved with sep=\";\" and encoding=\"utf-8-sig\"\n",
    "merged_df_clean = pd.read_csv(CLEAN_PATH, sep=\";\", encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Loaded:\", CLEAN_PATH)\n",
    "print(\"Shape:\", merged_df_clean.shape)\n",
    "\n",
    "# Choose identifier (varname is safest)\n",
    "ID_COL = \"varname\"\n",
    "\n",
    "total_objects = merged_df_clean[ID_COL].nunique()\n",
    "print(f\"\\nTotal unique objects: {total_objects}\\n\")\n",
    "\n",
    "def report_missing_object_level(label_col: str):\n",
    "    \"\"\"\n",
    "    Reports how many unique objects are missing a given label column.\n",
    "    Missing for an object = all rows for that object are NaN for that column.\n",
    "    \"\"\"\n",
    "    missing_by_object = (\n",
    "        merged_df_clean\n",
    "        .groupby(ID_COL)[label_col]\n",
    "        .apply(lambda s: s.isna().all())\n",
    "    )\n",
    "\n",
    "    num_missing = int(missing_by_object.sum())\n",
    "    percent_missing = 100 * num_missing / total_objects\n",
    "\n",
    "    print(f\"{label_col}:\")\n",
    "    print(f\"  Objects missing value : {num_missing}\")\n",
    "    print(f\"  Percentage missing    : {percent_missing:.2f}%\")\n",
    "    print(f\"  Objects with value    : {total_objects - num_missing}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Run reports on the NEW numeric targets\n",
    "report_missing_object_level(\"mbh_hbeta_only_log10_val\")\n",
    "report_missing_object_level(\"mbh_all_lines_log10_val\")\n",
    "report_missing_object_level(\"mbh_rm_modeling_log10_val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa62f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# # ----------------------------\n",
    "# # Load cleaned dataset\n",
    "# # ----------------------------\n",
    "# IN_PATH = Path(\"agnmass_dump/reverb_tables/qc_project_cleaned_data.csv\")\n",
    "# df = pd.read_csv(IN_PATH, sep=\";\", encoding=\"utf-8-sig\")\n",
    "# print(\"Loaded:\", IN_PATH, \"shape:\", df.shape)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Config / column names\n",
    "# # ----------------------------\n",
    "# ID_COL = \"varname\"          # or \"object_name\"\n",
    "# LINE_COL = \"Line\"\n",
    "\n",
    "# # Parsed numeric columns already in your cleaned data\n",
    "# TAU_COL = \"τcent (days)_val\"\n",
    "# SIGMA_COL = \"σline (km s-1)_val\"\n",
    "\n",
    "# # Existing Hβ-only label columns (parsed)\n",
    "# HB_LOG_COL = \"mbh_hbeta_only_log10_val\"\n",
    "# HB_LOG_PLUS = \"mbh_hbeta_only_log10_plus\"\n",
    "# HB_LOG_MINUS = \"mbh_hbeta_only_log10_minus\"\n",
    "\n",
    "# # If you want to also keep a Msun version\n",
    "# HB_MSUN_COL = \"mbh_hbeta_only_msun_val\"\n",
    "\n",
    "# # Output columns we will create\n",
    "# TARGET_LOG_COL = \"target_log10_mbh_hbeta\"\n",
    "# TARGET_SRC_COL = \"target_source\"   # 'website' vs 'computed'\n",
    "# TARGET_ROWFLAG = \"is_hbeta_row\"     # row is an Hβ measurement\n",
    "# TARGET_COMPUTED_FLAG = \"is_target_computed\"  # True if we computed due to missing website value\n",
    "\n",
    "# # Constants for computation\n",
    "# F_DEFAULT = 4.3  # as used by the site default\n",
    "# LOG10_C = np.log10(2.99792458e8)      # m/s\n",
    "# LOG10_G = np.log10(6.67430e-11)       # SI\n",
    "# LOG10_MSUN = np.log10(1.98847e30)     # kg\n",
    "# LOG10_DAY = np.log10(86400.0)         # s/day\n",
    "# LOG10_F = np.log10(F_DEFAULT)\n",
    "\n",
    "# # ----------------------------\n",
    "# # 1) Build Hβ-only target for all rows (website if present; computed if missing)\n",
    "# # ----------------------------\n",
    "\n",
    "# # Identify Hβ rows (the site uses \"Hβ\", sometimes with extra text like \"Hβ λ4861\")\n",
    "# df[TARGET_ROWFLAG] = df[LINE_COL].astype(str).str.contains(\"Hβ\", na=False)\n",
    "\n",
    "# # Start with website value if it exists\n",
    "# df[TARGET_LOG_COL] = df[HB_LOG_COL]\n",
    "\n",
    "# # Compute log10(MBH/Msun) for Hβ rows where website value is missing\n",
    "# # Formula:\n",
    "# #   MBH = f * (c * tau_sec) * (V_mps^2) / G\n",
    "# #   tau_sec = tau_days * 86400\n",
    "# #   V_mps = sigma_kmps * 1000\n",
    "# #\n",
    "# # In log10 space:\n",
    "# #   log10(MBH_kg) = log10(f) + log10(c) + log10(tau_days) + log10(86400)\n",
    "# #                  + 2*(log10(sigma_kmps) + 3) - log10(G)\n",
    "# #   log10(MBH/Msun) = log10(MBH_kg) - log10(Msun)\n",
    "\n",
    "# mask_need_compute = (\n",
    "#     df[TARGET_ROWFLAG]\n",
    "#     & df[TARGET_LOG_COL].isna()\n",
    "#     & df[TAU_COL].notna()\n",
    "#     & df[SIGMA_COL].notna()\n",
    "# )\n",
    "\n",
    "# # Safe logs\n",
    "# tau_days = df.loc[mask_need_compute, TAU_COL].astype(float)\n",
    "# sigma_kmps = df.loc[mask_need_compute, SIGMA_COL].astype(float)\n",
    "\n",
    "# computed_log10 = (\n",
    "#     LOG10_F\n",
    "#     + LOG10_C\n",
    "#     + np.log10(tau_days)\n",
    "#     + LOG10_DAY\n",
    "#     + 2.0 * (np.log10(sigma_kmps) + 3.0)  # km/s -> m/s = +3 in log10\n",
    "#     - LOG10_G\n",
    "#     - LOG10_MSUN\n",
    "# )\n",
    "\n",
    "# df.loc[mask_need_compute, TARGET_LOG_COL] = computed_log10\n",
    "\n",
    "# # Flags describing where target came from\n",
    "# df[TARGET_COMPUTED_FLAG] = False\n",
    "# df.loc[mask_need_compute, TARGET_COMPUTED_FLAG] = True\n",
    "\n",
    "# df[TARGET_SRC_COL] = \"website\"\n",
    "# df.loc[df[TARGET_COMPUTED_FLAG], TARGET_SRC_COL] = \"computed\"\n",
    "\n",
    "# # Optional: you may prefer to set non-Hβ rows' target to NaN (since they aren't Hβ measurements)\n",
    "# # If you want that, uncomment:\n",
    "# # df.loc[~df[TARGET_ROWFLAG], TARGET_LOG_COL] = np.nan\n",
    "\n",
    "# print(\"\\nAfter filling targets:\")\n",
    "# print(\"  Total rows:\", len(df))\n",
    "# print(\"  Hβ rows:\", int(df[TARGET_ROWFLAG].sum()))\n",
    "# print(\"  Computed targets (rows):\", int(df[TARGET_COMPUTED_FLAG].sum()))\n",
    "# print(\"  Missing target among Hβ rows:\", int(df.loc[df[TARGET_ROWFLAG], TARGET_LOG_COL].isna().sum()))\n",
    "\n",
    "# # ----------------------------\n",
    "# # 2) Save Version A: raw per-row targets (website or computed)\n",
    "# # ----------------------------\n",
    "# OUT_RAW = Path(\"agnmass_dump/reverb_tables/qc_project_hbeta_target_per_row.csv\")\n",
    "# df.to_csv(OUT_RAW, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "# print(\"\\nSaved per-row version to:\", OUT_RAW)\n",
    "\n",
    "# # ----------------------------\n",
    "# # 3) Version B: average computed targets per object (only for computed ones), then substitute\n",
    "# # ----------------------------\n",
    "\n",
    "# df_avg = df.copy()\n",
    "\n",
    "# # For each object, compute mean of computed targets across its Hβ rows\n",
    "# # (Only computed ones; website ones remain untouched.)\n",
    "# computed_means = (\n",
    "#     df_avg[df_avg[TARGET_COMPUTED_FLAG]]\n",
    "#     .groupby(ID_COL)[TARGET_LOG_COL]\n",
    "#     .mean()\n",
    "# )\n",
    "\n",
    "# # Substitute: for rows where target is computed, replace with that object's mean\n",
    "# df_avg.loc[df_avg[TARGET_COMPUTED_FLAG], TARGET_LOG_COL] = df_avg.loc[df_avg[TARGET_COMPUTED_FLAG], ID_COL].map(computed_means)\n",
    "\n",
    "# # Add a convenience flag indicating we performed averaging (still computed, but aggregated)\n",
    "# df_avg[\"is_target_computed_avg\"] = False\n",
    "# df_avg.loc[df_avg[TARGET_COMPUTED_FLAG], \"is_target_computed_avg\"] = True\n",
    "\n",
    "# OUT_AVG = Path(\"agnmass_dump/reverb_tables/qc_project_hbeta_target_computed_avg.csv\")\n",
    "# df_avg.to_csv(OUT_AVG, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "# print(\"Saved averaged-computed version to:\", OUT_AVG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c70f784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: agnmass_dump/reverb_tables/qc_project_cleaned_data.csv shape: (299, 77)\n",
      "\n",
      "After filling missing targets:\n",
      "  Rows before: 299\n",
      "  Rows after : 261\n",
      "  Dropped rows (no website label + no computable Hβ rows for that object): 38\n",
      "\n",
      "Object-level coverage:\n",
      "  Unique objects total: 86\n",
      "  Unique objects kept : 67\n",
      "\n",
      "Target source breakdown (rows):\n",
      "target_hbeta_source\n",
      "website    261\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved: agnmass_dump/reverb_tables/qc_finalized_data.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "IN_PATH = Path(\"agnmass_dump/reverb_tables/qc_project_cleaned_data.csv\")\n",
    "df = pd.read_csv(IN_PATH, sep=\";\", encoding=\"utf-8-sig\")\n",
    "print(\"Loaded:\", IN_PATH, \"shape:\", df.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# Columns\n",
    "# ----------------------------\n",
    "ID_COL = \"varname\"  # safest object id\n",
    "LINE_COL = \"Line\"\n",
    "\n",
    "TAU_COL = \"τcent (days)_val\"\n",
    "SIGMA_COL = \"σline (km s-1)_val\"\n",
    "\n",
    "# Website-derived parsed target (object-level metadata repeated on all rows)\n",
    "HB_WEBSITE_LOG = \"mbh_hbeta_only_log10_val\"\n",
    "\n",
    "# ----------------------------\n",
    "# Constants for virial computation\n",
    "# MBH = f * (c * tau) * (V^2) / G\n",
    "# tau in seconds, V in m/s\n",
    "# target = log10(MBH / Msun)\n",
    "# ----------------------------\n",
    "F_DEFAULT = 4.3\n",
    "C = 2.99792458e8         # m/s\n",
    "G = 6.67430e-11          # SI\n",
    "MSUN = 1.98847e30        # kg\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Start: object-level target from website if present\n",
    "# ----------------------------\n",
    "df[\"target_hbeta_log10\"] = df[HB_WEBSITE_LOG]\n",
    "df[\"target_hbeta_source\"] = np.where(df[\"target_hbeta_log10\"].notna(), \"website\", \"missing\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) For objects missing website Hβ target, try computing it from their Hβ rows\n",
    "# ----------------------------\n",
    "is_hbeta_row = df[LINE_COL].astype(str).str.contains(\"Hβ\", na=False)\n",
    "\n",
    "# Rows usable for computation\n",
    "usable = is_hbeta_row & df[TAU_COL].notna() & df[SIGMA_COL].notna()\n",
    "\n",
    "# Compute per-row virial log10(M/Msun) for usable Hβ rows\n",
    "tau_sec = df.loc[usable, TAU_COL].astype(float) * 86400.0\n",
    "v_mps = df.loc[usable, SIGMA_COL].astype(float) * 1000.0\n",
    "\n",
    "mbh_kg = F_DEFAULT * (C * tau_sec) * (v_mps ** 2) / G\n",
    "row_log10 = np.log10(mbh_kg / MSUN)\n",
    "\n",
    "# Store the per-row computed values (only for Hβ rows where we can compute)\n",
    "df[\"hbeta_row_computed_log10\"] = np.nan\n",
    "df.loc[usable, \"hbeta_row_computed_log10\"] = row_log10\n",
    "\n",
    "# Compute an object-level mean from available computed Hβ rows\n",
    "computed_obj_mean = (\n",
    "    df.loc[usable]\n",
    "      .groupby(ID_COL)[\"hbeta_row_computed_log10\"]\n",
    "      .mean()\n",
    ")\n",
    "\n",
    "# Fill missing target_hbeta_log10 at the OBJECT level using that mean\n",
    "missing_obj = df[\"target_hbeta_log10\"].isna()\n",
    "df.loc[missing_obj, \"target_hbeta_log10\"] = df.loc[missing_obj, ID_COL].map(computed_obj_mean)\n",
    "\n",
    "# Update source flag\n",
    "df.loc[df[\"target_hbeta_source\"].eq(\"missing\") & df[\"target_hbeta_log10\"].notna(), \"target_hbeta_source\"] = \"computed_obj_mean\"\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Now drop ONLY rows whose object still has no Hβ target (website missing AND cannot compute)\n",
    "# ----------------------------\n",
    "before = df.shape[0]\n",
    "df_final = df.dropna(subset=[\"target_hbeta_log10\"]).copy()\n",
    "dropped = before - df_final.shape[0]\n",
    "\n",
    "print(\"\\nAfter filling missing targets:\")\n",
    "print(\"  Rows before:\", before)\n",
    "print(\"  Rows after :\", df_final.shape[0])\n",
    "print(\"  Dropped rows (no website label + no computable Hβ rows for that object):\", dropped)\n",
    "\n",
    "print(\"\\nObject-level coverage:\")\n",
    "print(\"  Unique objects total:\", df[ID_COL].nunique())\n",
    "print(\"  Unique objects kept :\", df_final[ID_COL].nunique())\n",
    "\n",
    "print(\"\\nTarget source breakdown (rows):\")\n",
    "print(df_final[\"target_hbeta_source\"].value_counts(dropna=False))\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Save with comma delimiter\n",
    "# ----------------------------\n",
    "OUT_PATH = Path(\"agnmass_dump/reverb_tables/qc_finalized_data.csv\")\n",
    "df_final.to_csv(OUT_PATH, index=False, sep=\",\", encoding=\"utf-8-sig\")\n",
    "print(\"\\nSaved:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e8bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qc_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
